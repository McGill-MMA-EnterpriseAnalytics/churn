\documentclass[12pt]{article}

\usepackage{amssymb,amsmath,amsfonts,eurosym,geometry,ulem,graphicx,caption,color,setspace,sectsty,comment,footmisc,caption,natbib,pdflscape,subfigure,array,booktabs,float}
\usepackage[hidelinks]{hyperref}
\normalem

\onehalfspacing
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newtheorem{hyp}{Hypothesis}
\newtheorem{subhyp}{Hypothesis}[hyp]
\renewcommand{\thesubhyp}{\thehyp\alph{subhyp}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\arraybackslash\hspace{0pt}}m{#1}}

\geometry{left=1.0in,right=1.0in,top=1.0in,bottom=1.0in}

\begin{document}

\begin{titlepage}
\title{TelCo Customer Churn Prediction\\
  \large INSY-695-076 Enterprise Analytics Final Project Report}
\author{Devanshu Khurma\thanks{260894480}\\
\and Jiajun Huang\thanks{260629217}\\
\and Charlie Cai\thanks{260926881}\\
\and Evelyn Sun\thanks{260915480}}
\date{\today}
\maketitle
\setcounter{page}{0}
\thispagestyle{empty}
\end{titlepage}
\pagebreak \newpage
\tableofcontents



\doublespacing

\newpage
\section{Introduction} \label{sec:introduction}

The core objective is predicting customer churn behavior for the TelCo Company. TelCo Company currently has no data-driven and quantitatively-based prediction model. Our team will develop machine learning models, using a 7043-observation (customer) with 20 predictors dataset. The model will be a supervised, binary classification model, since we have a clear binary target variable, churn or no churn. To achieve the optimal outcome, our team will both manually build machine learning models in Python and develop an Auto Machine Learning Model in H20.ai. The core performance will be measured in terms of the accuracy and F1 Score.

After the optimal model was chosen and picked, it will be run off-line periodically (at most once a day). In other words, the prediction model will build on a batch processing system using Apache Spark, which takes a large amount of input data, runs the pickled prediction model to process it, and produces the prediction outcome. 

Our model will be used by the marketing and consumer retention team so that they could intervene early by either offering targeted online offers and coupons, or having representatives phoning those consumers who are predicted to churn. Also, the model explain-ability report will be used by middle-level managers to gain insights on the factor that both positively and negatively contribute to the churn rate so they could address such factors accordingly. 

\subsection{Project Hypothesis}
The goal of our project is to predict (i.e., classifying) whether a current customer will churn from TelCo or not.

$H_0$(Null Hypothesis): The accuracy score is less or equal to 0.7 and no predicting power can be generated from the model. 

$H_\alpha$(Alternative Hypothesis): The accuracy score is greater than 0.7 and some predicting power can be generated from the model.

Types of Errors: Both Type I and Type II errors are expected to be made by the model. The goal of the model is to minimize Type II errors (i.e. reducing false negative rate.)    

\subsection{Core Actions}
\begin{itemize}
	\item Trained multiple Python-based Machine Learning models to predict customer churn for the TelCo Company
	\item Programmed a H20.ai-based Auto Machine Learning model and compared the AutoML model performance (in terms of f1 score and accuracy) against manually trained models
	\item Tested causal inference on following predictors: gender, SeniorCitizen, Partner, Dependents, PhoneService, and PaperlessBilling by applying Microsoft DoWhy package realized in Python
	\item Assembled a model interpretability and explainability analysis using SHAP package on XGBoost Classification Model
	\item Composed a Machine Learning Bias Report based on the SHAR analysis to suggest further managerial actions
	\item Lessons learned and next steps

\end{itemize}

\section{Data Description} \label{sec:DataDescription}

\subsection{Data Acquisition}

We used the data from Kaggle’s Telco Customer Churn prediction challenge. The data has 7043 profiles of customers along with labels representing whether they churned or not.

The original dataset was 955 kb in size. We created a folder for the dataset after every stage of processing. We downloaded the data and stored in an csv file.

In addition, to ensure sensitive information is deleted or protected we removed the customer ID column during pre-processing data(e.g. anonymized). After that, 2e looked at the types of data and dealt with all the categorical variable and made sure that there was no data snooping.

\subsection{Data Cleaning}
In the data cleaning part, we looked at the number of missing values and surprisingly, found none. Then, we started of looking at the data types and counting features of each type, it
turns out that 18/21 columns were categorical, 2 were integers and 1 was float. 

Next, we looked at the number of different values there can exist in each of the categorical feature columns. It is noticed an issue here because TotalCharges column representing the total amount charged to the customer was a string and not a float like it should be, 
so we converted it to a float and realized that there were now some missing values for float which we replaced by 0 assuming the person was not charged anything so far.

In the last step, we dummified the categorical variables and moved on to exploration.

\subsection{Data Exploration}
In this step, we created a Jupyter notebook to keep a record of our data exploration.

first, we looked at summary statistics of the numerical columns to find the mean and standard deviation of customers who churned and did not churn. There was not much difference between the mean values of both classes’ mean value for Monthly Charges. The variance in Monthly Charges for those who churned was about 33.33\% the value of the mean while for those who did not it was about 50\% of the mean value indicating the loyal customers showed higher diversity in the amount they paid monthly.

We then looked at the top features showing the highest the lowest correlation with the outcome variables. Monthly charges were positively  correlated while Total Charges and Tenure were negatively correlated with Tenure most negatively correlated with the customer churning as expected. This indicated that the longer the person stays the less likely he is to ever churn.

To better understand the correlation, we visualized the correlation matrix, and followed this by experimenting with various models to predict the churn.

\subsection{Data Preparation}
\begin{itemize}
	\item Dealt with missing data in TotalCharges column
	\item Encoding of categorical variables
	\item Feature subset selection
	\item Feature scaling
	\item  After the preparation, there were no missing values and all categorical variables were dummified and all numerical variables are normalized  by using an sklearn-pipeline.
\end{itemize}


\section{Churn Prediction Modeling} \label{sec:model}
\subsection{Python-based Model }

We experimented with various different classifier algorithms including random forests, decision trees, logistic regression, K Nearest Neighbors and even XG Boost.

The performance from Random Forests and XG Boost was most promising so we used grid search CV to fine tune these 2 models.

We got the best performance from the random forest with accuracy of 84.9\% and an F1 score of 0.56

\subsection{H20 Auto-ML Model}
H2O’s AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. It has made it easy for non-experts to experiment with machine learning to set a benchmark. 

We have used H2O package to build models to compare the results with our Python-based model. 

Due to the limited available algorithms at this stage of H2O, we were only able to perform Random Forest and Gradient Boosting to compare with our Python-based model. However, we also explored deep learning, and asked H2O to self-select 10 best performing models.

\begin{table}[htbp]
  \centering
  \caption{H2O AutoML Model Results}
    \begin{tabular}{lrr}
    \toprule
    Model  & \multicolumn{1}{l}{Accuracy Score} & \multicolumn{1}{l}{F1 Score} \\
    \midrule
    Random Forest & 0.8272 & 0.630814 \\
    Gradient Boosting & 0.83457 & 0.627168 \\
    Neutral Network & 0.82574 & 0.619318 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}\\%
\end{table}%

\begin{table}[htbp]
  \centering
  \caption{10 Best Performing Models Selected by H2O AutoML}
    \begin{tabular}{lrr}
    \toprule
    model\_id & \multicolumn{1}{l}{auc} & \multicolumn{1}{l}{mse} \\
    \midrule
    StackedEnsemble\_BestOfFamily\_AutoML\_20200216\_115946 & 0.850544 & 0.133636 \\
    StackedEnsemble\_AllModels\_AutoML\_20200216\_115946 & 0.850509 & 0.133678 \\
    GBM\_5\_AutoML\_20200216\_115946 & 0.848467 & 0.13321 \\
    GLM\_1\_AutoML\_20200216\_115946 & 0.847801 & 0.133984 \\
    GBM\_grid\_\_1\_AutoML\_20200216\_115946\_model\_1 & 0.842607 & 0.148619 \\
    GBM\_1\_AutoML\_20200216\_115946 & 0.841779 & 0.136873 \\
    GBM\_2\_AutoML\_20200216\_115946 & 0.841048 & 0.136614 \\
    GBM\_3\_AutoML\_20200216\_115946 & 0.838457 & 0.138417 \\
    DeepLearning\_1\_AutoML\_20200216\_115946 & 0.83469 & 0.13864 \\
    GBM\_4\_AutoML\_20200216\_115946 & 0.832629 & 0.140648 \\
    XRT\_1\_AutoML\_20200216\_115946 & 0.832395 & 0.140071 \\
    DRF\_1\_AutoML\_20200216\_115946 & 0.828687 & 0.141467 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

As we can see from Table 1 and Table 2, H2O AutoML has produced high performance models with accuracy scores are all more than 80\%, in this case, we REJECT $H_0$, which means that models built by H2O AutoML have prediction po
\subsection{Comparison of Python-Basedwer.
 Models and H2O AutoML}
\begin{figure}[H]
\centering
\includegraphics[scale=1.2]{f1.jpg}
\caption{Model Result Comparison}\label{fig:}
\end{figure}

From Figure 1, we can see that python based Tunned Random Forest delivers the highest accuracy, 0.85, but for Gradient Boosting and XG Boost, H2O AutoML actually produces better result.

\section{Further Discussion and Interpretation} \label{sec:Interpretation}
\subsection{Casual Inference} \label{sec:Casuality}

We tested causal inference on following predictors: Gender, Senior Citizen, Partner, Dependents, PhoneService, and Paperless Billing by applying Microsoft DoWhy package realized in Python. Results are shown in the following Table 3. 
\begin{table}[htbp]
  \centering
  \caption{Casual Inference Report }
    \begin{tabular}{lrr}
    \toprule
    Variables                          & \multicolumn{1}{l}{  P-Value } & \multicolumn{1}{l}{ Causal Estimate } \\
    \midrule
     Contract Month-to-month            & \multicolumn{1}{l}{ $<0.001$   } & 0.05997 \\
     Paperless Billing                  & \multicolumn{1}{l}{ $<0.001$   } & 0.04491 \\
     Gender                             & 0.358 & -0.00329 \\
     Senior Citizen                     & 0.044 & 0.0442 \\
     Partner                            & 0.464 & -0.00122 \\
     Dependents                         & 0.065 & -0.02092 \\
     Phone Service                      & 0.424 & 0.00988 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

We found that only Contract Month-to-month and Paperless Billing have causal relationship with the target. However, we cannot draw conclusion lightly based solely on the result. Interpretability and explainability report is needed to further investigate the relationship.

\subsection{Churn Model Interpretability and Explainability} \label{sec:Interpretability}

To access the XGBoost Classification Model interpretability and explainability, we used the SHAR package to visualize the predictors' effect on the target variable, churn. SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. The reason we choose the XGBoost Classification Model to analyze instead of the Random Forest Model because the Random Forest Model takes significantly longer compared to the XGBoost Model, and our team's laptops are unable to provide the results. 

We first can to visualize the first prediction's explanation in Figure 2.

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.8]{f2.jpg}
\caption{}\label{fig:}
\end{figure}

Next, we summarize the effects of all the features. The plot below sorts features by the sum of SHAP value, and uses SHAP values to show the distribution of the impacts each feature has on the model output. A higher feature value is colored in red and a lower feature value is colored in blue. 

\begin{figure}[htbp]
\centering
\includegraphics[scale=1]{f3.jpg}
\caption{}\label{fig:}
\end{figure}

Here are some sample interpretations"
\begin{itemize}
	\item If a consumer is on a month to month contract (red-colored), it is more likely to churn
	\item The longer a consumer is with the company (higher red-colored for tenure), it is less likely to churn
	\item If a consumer's monthly charges are high, it is more likely to churn
	\item On the other hand, if the total charges are high, it is less likely to churn. This could imply that for high spenders at TelCo, these consumers are less price sensitive
	\item If a consumer is a senior citizen, it is more lily to churn
\end{itemize}

We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):
\begin{figure}[htbp]
\centering
\includegraphics[scale=1]{f4.jpg}
\caption{}\label{fig:}
\end{figure}

\subsection{Threats to Validity}

There are four primary considerations for data quality and threats to validity. 

First, the data consist of only 7000 observations. The results we have are acceptable but are not outstanding. To achieve more desirable results, we recommend the TelCo to add more observations to train the model. 

Secondly, the data provided is a snapshot of the past data at a certain point in time. However, as the market and competitors change, the customer churn will change as well. Hence we recommended the data analyst team the TelCo continuously train the model. 

Thirdly, the model does not consider external factors, such as macro-economic factors. 

Lastly, regarding the depth of the data, the data does not contain customer satisfaction levels or user engagement factors. We recommend the TelCo to explore more variables to include in the dataset. 

\section{Conclusion} \label{sec:conclusion}
\subsection{Model Bias}

There are four predictors used in the model that may be considered as potential biased or discriminatory, namely, gender, senior citizen (i.e., whether the customer is a senior citizen or not), partner (i.e., whether the customer has a partner or not), and dependents (whether the customer has dependents or not). 

Based on the Gini coefficients report and summarization effects of all the features, only senior citizens and gender show a high degree of effects on model prediction outcome. This implies that we can still keep partners and dependents in our model without worrying about model bias. 

Regarding gender, the predictor's effect has a low feature value and low mean absolute SHAP values. Hence, by dropping the gender from the model, not only can we prevent any potential model bias, but also the adverse effects on the model performance by dropping the predictor is lowest. If we drop the predictor, we may lose significant predicting power. 

Additionally, age or date of birth predictors is widely used in machine learning models such as credit card approval and insurance underwriting. Hence, we would recommend keeping the senior citizen predictor.

\subsection{Further Research}

Our team would recommend the marketing analysis at the TelCo continue to investigate the model interpretability report, to identify churn correlations or causes, and to address the issues accordingly. For example, our team has found that if a consumer is with a two-year phone contract, then that consumer is less likely to churn. Hence, the TelCo marketing team could offer better new two-year deals to attract new customers to sign-up for a new two-year contract. However, some factors that contribute to the prediction outcome are not immediately apparent. For example, our team has discovered that if a consumer signed up for paperless billing, it is more likely to churn. The TelCo team cannot derive action such as stop all paperless billing based on the report alone. Instead the TelCo team should deep dive into the causal inference report and discover the unobserved factors, for example, what types of people are more likely to sign-up for paperless billing.  


\singlespacing
\setlength\bibsep{0pt}
\bibliographystyle{my-style}
\bibliography{Placeholder}


\end{document}